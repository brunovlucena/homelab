apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: loki-alerts
  namespace: loki
  labels:
    app.kubernetes.io/name: loki
    app.kubernetes.io/component: monitoring
    app.kubernetes.io/part-of: homelab
    prometheus: kube-prometheus
    release: prometheus-operator
spec:
  groups:
  # =============================================================================
  # üö® LOKI STORAGE & INGESTION ALERTS
  # =============================================================================
  - name: loki.storage
    interval: 30s
    rules:
    # üî¥ CRITICAL: Loki chunk flush failures (S3/MinIO storage errors)
    - alert: LokiChunkFlushFailures
      expr: rate(loki_ingester_chunks_flush_failures_total[5m]) > 0
      for: 2m
      labels:
        severity: critical
        component: loki-ingester
        category: storage
        team: sre
      annotations:
        summary: "üî¥ Loki is failing to flush chunks to storage"
        description: |
          Loki ingester {{ $labels.pod }} is experiencing chunk flush failures at {{ $value | humanize }} failures/sec.
          
          **Common Causes:**
          - MinIO bucket does not exist (NoSuchBucket error)
          - MinIO is down or unreachable
          - S3 credentials are incorrect
          - Network connectivity issues
          - Disk space exhausted on MinIO
          
          **Impact:**
          - Log data is being buffered in memory and WAL
          - Risk of data loss if ingester restarts
          - Increased memory pressure on write pods
          
          **Current State:**
          Queue Length: Check loki_ingester_flush_queue_length
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/loki/loki-storage-issues.md#issue-chunk-flush-failures---nosuchbucket-errors"
        dashboard_url: "https://grafana.bruno.com/d/loki-writes"
    
    # üü° WARNING: High flush queue length
    - alert: LokiHighFlushQueueLength
      expr: loki_ingester_flush_queue_length > 100
      for: 5m
      labels:
        severity: warning
        component: loki-ingester
        category: storage
        team: sre
      annotations:
        summary: "‚ö†Ô∏è Loki flush queue is growing"
        description: |
          Loki ingester {{ $labels.pod }} has {{ $value }} chunks queued for flushing.
          
          **Possible Causes:**
          - Slow storage backend (MinIO)
          - High ingestion rate
          - Insufficient write throughput
          - Network latency to storage
          
          **Action Required:**
          - Monitor for continued growth
          - Check MinIO performance
          - Verify storage backend health
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/loki/loki-storage-issues.md#issue-chunk-flush-failures---nosuchbucket-errors"
    
    # üî¥ CRITICAL: Flush queue critically high
    - alert: LokiFlushQueueCritical
      expr: loki_ingester_flush_queue_length > 500
      for: 2m
      labels:
        severity: critical
        component: loki-ingester
        category: storage
        team: sre
      annotations:
        summary: "üî¥ CRITICAL: Loki flush queue critically high"
        description: |
          Loki ingester {{ $labels.pod }} has {{ $value }} chunks queued for flushing!
          
          **IMMEDIATE ACTION REQUIRED:**
          1. Check MinIO health: `kubectl get pods -n loki -l app.kubernetes.io/name=minio`
          2. Verify bucket exists: Check MinIO console or logs
          3. Check storage space: MinIO disk usage
          4. Review ingester logs: `kubectl logs -n loki {{ $labels.pod }}`
          
          **Risk:**
          - High memory consumption
          - Potential OOM kills
          - Data loss if pod restarts
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/loki/loki-storage-issues.md#issue-chunk-flush-failures---nosuchbucket-errors"
    
  # =============================================================================
  # üîß LOKI INGESTER HEALTH
  # =============================================================================
  - name: loki.ingester
    interval: 30s
    rules:
    # Loki Write Path Down
    - alert: LokiWritePathDown
      expr: up{job="loki/loki-write"} == 0
      for: 1m
      labels:
        severity: critical
        component: loki-write
        category: availability
        team: sre
      annotations:
        summary: "üî¥ Loki write path is down"
        description: |
          Loki write component {{ $labels.pod }} is not responding.
          
          **Impact:**
          - No new logs are being ingested
          - Applications cannot send logs
          - Log pipeline is broken
          
          **Action:**
          Check pod status: `kubectl get pods -n loki -l app.kubernetes.io/component=write`
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/loki/loki-write-path-down.md"
    
    # Loki Read Path Down
    - alert: LokiReadPathDown
      expr: up{job="loki/loki-read"} == 0
      for: 2m
      labels:
        severity: warning
        component: loki-read
        category: availability
        team: sre
      annotations:
        summary: "‚ö†Ô∏è Loki read path is down"
        description: |
          Loki read component {{ $labels.pod }} is not responding.
          
          **Impact:**
          - Cannot query logs
          - Grafana Explore won't work
          - Historical log access unavailable
          
          **Action:**
          Check pod status: `kubectl get pods -n loki -l app.kubernetes.io/component=read`
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/loki/loki-read-path-slow.md"
    
  # =============================================================================
  # üíæ LOKI STORAGE BACKEND (MinIO)
  # =============================================================================
  - name: loki.minio
    interval: 30s
    rules:
    # MinIO for Loki is down
    - alert: LokiMinIODown
      expr: up{job="loki/loki-minio"} == 0
      for: 1m
      labels:
        severity: critical
        component: minio
        category: storage
        team: sre
      annotations:
        summary: "üî¥ CRITICAL: Loki's MinIO storage is down"
        description: |
          MinIO storage backend for Loki is not responding.
          
          **IMMEDIATE IMPACT:**
          - Cannot flush chunks to storage (causing the NoSuchBucket errors)
          - Cannot query historical logs
          - Risk of data loss if ingesters restart
          
          **Action:**
          1. Check MinIO pods: `kubectl get pods -n loki -l app.kubernetes.io/name=minio`
          2. Check MinIO logs: `kubectl logs -n loki -l app.kubernetes.io/name=minio`
          3. Verify PVC: `kubectl get pvc -n loki`
          4. Check disk space on node
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/loki/loki-storage-issues.md"

  # =============================================================================
  # üìù LOKI UPGRADE & HELMRELEASE MONITORING
  # =============================================================================
  - name: loki.upgrades
    interval: 30s
    rules:
    # üü° WARNING: Loki frequent upgrades detected
    - alert: LokiFrequentUpgrades
      expr: |
        rate(kube_helmrelease_status_condition{name="loki", namespace="loki", type="Reconciling", status="True"}[10m]) > 0.1
        or
        increase(kube_helmrelease_status_condition{name="loki", namespace="loki", type="Reconciling", status="True"}[1h]) > 3
      for: 5m
      labels:
        severity: warning
        component: loki
        category: helmrelease
        team: sre
        service: observability
      annotations:
        summary: "‚ö†Ô∏è Loki experiencing frequent upgrades"
        description: |
          Loki HelmRelease is being upgraded more frequently than expected.
          
          **Current State:**
          - Upgrade rate: {{ $value | humanize }} upgrades/minute
          - Recent upgrades: {{ $value }} in the last hour
          
          **Common Causes:**
          - Configuration value changes being detected repeatedly
          - Secret value changes (MinIO credentials)
          - Resource constraint changes
          - Node selector changes
          - Storage configuration updates
          
          **Impact:**
          - Potential service disruption during upgrades
          - Increased resource usage
          - Log ingestion may be temporarily unavailable
          
          **Action Required:**
          1. Check HelmRelease status: `kubectl describe helmrelease loki -n loki`
          2. Review recent changes: `kubectl get events -n loki --sort-by='.lastTimestamp'`
          3. Check for configuration drift: Compare current values with desired state
          4. Monitor for pattern in upgrade triggers
          5. Consider stabilizing configuration values
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/loki/loki-frequent-upgrades.md"
        dashboard_url: "https://grafana.bruno.com/d/loki-overview"
    
    # üî¥ CRITICAL: Loki upgrade failures
    - alert: LokiUpgradeFailures
      expr: |
        kube_helmrelease_status_condition{name="loki", namespace="loki", type="Ready", status="False"} == 1
        and
        kube_helmrelease_status_condition{name="loki", namespace="loki", type="Ready", reason="Failed"} == 1
      for: 3m
      labels:
        severity: critical
        component: loki
        category: helmrelease
        team: sre
        service: observability
      annotations:
        summary: "üî¥ CRITICAL: Loki upgrade has failed"
        description: |
          Loki HelmRelease upgrade has failed and is in a failed state.
          
          **Error Details:**
          - Release: {{ $labels.name }}
          - Namespace: {{ $labels.namespace }}
          - Status: {{ $labels.status }}
          - Reason: {{ $labels.reason }}
          
          **Common Causes:**
          - Storage configuration issues (MinIO connectivity)
          - Resource constraints during upgrade
          - Configuration validation failures
          - Dependency issues
          
          **Impact:**
          - Log aggregation is unavailable
          - Observability stack is incomplete
          - Applications cannot send logs
          
          **Action Required:**
          1. Check HelmRelease status: `kubectl describe helmrelease loki -n loki`
          2. Check Loki pods: `kubectl get pods -n loki`
          3. Review upgrade logs: `kubectl logs -n flux-system -l app=flux`
          4. Check MinIO connectivity: `kubectl get pods -n loki -l app.kubernetes.io/name=minio`
          5. Verify storage configuration and credentials
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/loki/loki-upgrade-failed.md"
        dashboard_url: "https://grafana.bruno.com/d/loki-overview"
    
    # üü° WARNING: Loki configuration drift detected
    - alert: LokiConfigurationDrift
      expr: |
        kube_helmrelease_status_condition{name="loki", namespace="loki", type="Reconciling", status="True"} == 1
        and
        kube_helmrelease_status_condition{name="loki", namespace="loki", type="Ready", status="False"} == 1
      for: 15m
      labels:
        severity: warning
        component: loki
        category: configuration
        team: sre
        service: observability
      annotations:
        summary: "‚ö†Ô∏è Loki configuration drift detected"
        description: |
          Loki HelmRelease has been reconciling for more than 15 minutes, indicating potential configuration drift.
          
          **Possible Causes:**
          - Configuration values are changing repeatedly
          - Secret values are being updated frequently
          - Resource constraints are causing reconciliation loops
          - Node scheduling issues
          
          **Action:**
          1. Check for configuration changes: `kubectl get helmrelease loki -n loki -o yaml`
          2. Review recent events: `kubectl get events -n loki --sort-by='.lastTimestamp'`
          3. Check for resource constraints
          4. Monitor for pattern in reconciliation triggers
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/loki/loki-configuration-drift.md"

