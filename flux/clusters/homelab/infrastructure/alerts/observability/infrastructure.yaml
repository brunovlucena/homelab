---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: infrastructure-monitoring
  namespace: prometheus
  labels:
    app.kubernetes.io/name: infrastructure-monitoring
    app.kubernetes.io/component: prometheus-rules
    app.kubernetes.io/part-of: homelab
    prometheus: kube-prometheus
    release: prometheus-operator
spec:
  groups:
  # =============================================================================
  # üö® TEMPO HELMRELEASE MONITORING
  # =============================================================================
  - name: tempo.helmrelease
    interval: 30s
    rules:
    # üî¥ CRITICAL: Tempo HelmRelease in failed state
    - alert: TempoHelmReleaseFailed
      expr: |
        (
          kube_helmrelease_status_condition{name="tempo", namespace="tempo", type="Ready", status="False"} == 1
          and
          kube_helmrelease_status_condition{name="tempo", namespace="tempo", type="Ready", reason="Failed"} == 1
        )
        or
        (
          kube_helmrelease_status_condition{name="tempo", namespace="tempo", type="Reconciling", status="False"} == 1
          and
          kube_helmrelease_status_condition{name="tempo", namespace="tempo", type="Reconciling", reason="Failed"} == 1
        )
      for: 2m
      labels:
        severity: critical
        component: tempo
        category: helmrelease
        team: sre
        service: observability
      annotations:
        summary: "üî¥ CRITICAL: Tempo HelmRelease is in failed state"
        description: |
          Tempo HelmRelease has been in a failed state for more than 2 minutes.
          
          **Error Details:**
          - Release: {{ $labels.name }}
          - Namespace: {{ $labels.namespace }}
          - Status: {{ $labels.status }}
          - Reason: {{ $labels.reason }}
          
          **Common Causes:**
          - "terminal error: exceeded maximum retries: cannot remediate failed release"
          - "uninstall: Failed to purge the release: release: not found"
          - Resource conflicts or dependency issues
          - Storage/PVC problems
          - Node scheduling issues
          
          **Impact:**
          - Tempo distributed tracing is unavailable
          - Observability stack is incomplete
          - Application traces cannot be collected
          
          **Action Required:**
          1. Check HelmRelease status: `kubectl describe helmrelease tempo -n tempo`
          2. Check Tempo pods: `kubectl get pods -n tempo`
          3. Review Flux logs: `kubectl logs -n flux-system -l app=flux`
          4. Check for resource conflicts or storage issues
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/tempo/tempo-helmrelease-failed.md"
        dashboard_url: "https://grafana.bruno.com/d/tempo-overview"
    
    # üü° WARNING: Tempo HelmRelease stuck in reconciling
    - alert: TempoHelmReleaseStuckReconciling
      expr: |
        kube_helmrelease_status_condition{name="tempo", namespace="tempo", type="Reconciling", status="True"} == 1
        and
        kube_helmrelease_status_condition{name="tempo", namespace="tempo", type="Ready", status="False"} == 1
      for: 10m
      labels:
        severity: warning
        component: tempo
        category: helmrelease
        team: sre
        service: observability
      annotations:
        summary: "‚ö†Ô∏è Tempo HelmRelease stuck in reconciling state"
        description: |
          Tempo HelmRelease has been reconciling for more than 10 minutes without becoming ready.
          
          **Possible Causes:**
          - Slow resource provisioning
          - Image pull issues
          - Resource constraints
          - Network connectivity problems
          
          **Action:**
          - Monitor for progression to failed state
          - Check resource availability
          - Review pod events and logs
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/tempo/tempo-helmrelease-stuck.md"
    
    # üî¥ CRITICAL: Tempo pods not running
    - alert: TempoPodsNotRunning
      expr: |
        kube_pod_status_phase{namespace="tempo", pod=~"tempo-.*", phase!="Running"} == 1
        or
        up{job="tempo", namespace="tempo"} == 0
      for: 3m
      labels:
        severity: critical
        component: tempo
        category: availability
        team: sre
        service: observability
      annotations:
        summary: "üî¥ CRITICAL: Tempo pods are not running"
        description: |
          Tempo pods are not in Running state or not responding to health checks.
          
          **Impact:**
          - Distributed tracing is completely unavailable
          - Applications cannot send traces
          - Observability stack is broken
          
          **Action:**
          1. Check pod status: `kubectl get pods -n tempo`
          2. Check pod events: `kubectl describe pod -n tempo <pod-name>`
          3. Check pod logs: `kubectl logs -n tempo <pod-name>`
          4. Verify resource constraints and node capacity
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/tempo/tempo-pods-down.md"

  # =============================================================================
  # üê∞ RABBITMQ OPERATOR MONITORING
  # =============================================================================
  - name: rabbitmq.operator
    interval: 30s
    rules:
    # üî¥ CRITICAL: RabbitMQ operator HelmRelease failed
    - alert: RabbitMQOperatorHelmReleaseFailed
      expr: |
        (
          kube_helmrelease_status_condition{name="rabbitmq-operator", namespace="rabbitmq-operator", type="Ready", status="False"} == 1
          and
          kube_helmrelease_status_condition{name="rabbitmq-operator", namespace="rabbitmq-operator", type="Ready", reason="Failed"} == 1
        )
        or
        (
          kube_helmrelease_status_condition{name="rabbitmq-operator", namespace="rabbitmq-operator", type="Reconciling", status="False"} == 1
          and
          kube_helmrelease_status_condition{name="rabbitmq-operator", namespace="rabbitmq-operator", type="Reconciling", reason="Failed"} == 1
        )
      for: 2m
      labels:
        severity: critical
        component: rabbitmq-operator
        category: helmrelease
        team: sre
        service: messaging
      annotations:
        summary: "üî¥ CRITICAL: RabbitMQ operator HelmRelease failed"
        description: |
          RabbitMQ operator HelmRelease has failed to install or is in a failed state.
          
          **Error Details:**
          - Release: {{ $labels.name }}
          - Namespace: {{ $labels.namespace }}
          - Status: {{ $labels.status }}
          - Reason: {{ $labels.reason }}
          
          **Common Causes:**
          - "release not installed: no release in storage for object"
          - "terminal error: exceeded maximum retries: cannot install release"
          - CRD installation failures
          - Resource conflicts with existing installations
          - Insufficient permissions
          
          **Impact:**
          - RabbitMQ clusters cannot be managed
          - Message queuing infrastructure unavailable
          - Event-driven applications affected
          
          **Action Required:**
          1. Check HelmRelease status: `kubectl describe helmrelease rabbitmq-operator -n rabbitmq-operator`
          2. Check operator pods: `kubectl get pods -n rabbitmq-operator`
          3. Review CRD installation: `kubectl get crd | grep rabbitmq`
          4. Check for resource conflicts: `kubectl get all -n rabbitmq-operator`
          5. Review Flux logs: `kubectl logs -n flux-system -l app=flux`
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/rabbitmq/rabbitmq-operator-failed.md"
        dashboard_url: "https://grafana.bruno.com/d/rabbitmq-overview"
    
    # üü° WARNING: RabbitMQ operator pods not ready
    - alert: RabbitMQOperatorPodsNotReady
      expr: |
        kube_pod_status_phase{namespace="rabbitmq-operator", pod=~"rabbitmq-cluster-operator-.*", phase!="Running"} == 1
        or
        kube_pod_status_ready{namespace="rabbitmq-operator", pod=~"rabbitmq-cluster-operator-.*", condition="Ready"} == 0
      for: 5m
      labels:
        severity: warning
        component: rabbitmq-operator
        category: availability
        team: sre
        service: messaging
      annotations:
        summary: "‚ö†Ô∏è RabbitMQ operator pods not ready"
        description: |
          RabbitMQ operator pods are not in Running state or not ready.
          
          **Impact:**
          - RabbitMQ cluster management unavailable
          - New RabbitMQ clusters cannot be created
          - Existing clusters may not be properly managed
          
          **Action:**
          1. Check pod status: `kubectl get pods -n rabbitmq-operator`
          2. Check pod events: `kubectl describe pod -n rabbitmq-operator <pod-name>`
          3. Check pod logs: `kubectl logs -n rabbitmq-operator <pod-name>`
          4. Verify resource constraints
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/rabbitmq/rabbitmq-operator-pods-not-ready.md"
    
    # üî¥ CRITICAL: RabbitMQ operator completely down
    - alert: RabbitMQOperatorDown
      expr: up{job="rabbitmq-operator", namespace="rabbitmq-operator"} == 0
      for: 3m
      labels:
        severity: critical
        component: rabbitmq-operator
        category: availability
        team: sre
        service: messaging
      annotations:
        summary: "üî¥ CRITICAL: RabbitMQ operator is down"
        description: |
          RabbitMQ operator is not responding to health checks.
          
          **Impact:**
          - Complete loss of RabbitMQ cluster management
          - Cannot create or manage RabbitMQ clusters
          - Event-driven architecture is broken
          
          **Action:**
          1. Check operator deployment: `kubectl get deployment -n rabbitmq-operator`
          2. Check operator logs: `kubectl logs -n rabbitmq-operator -l app.kubernetes.io/name=rabbitmq-cluster-operator`
          3. Verify operator configuration
          4. Check for resource constraints or node issues
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/rabbitmq/rabbitmq-operator-down.md"

  # =============================================================================
  # üìù LOKI UPGRADE MONITORING
  # =============================================================================
  - name: loki.upgrades
    interval: 30s
    rules:
    # üü° WARNING: Loki frequent upgrades detected
    - alert: LokiFrequentUpgrades
      expr: |
        rate(kube_helmrelease_status_condition{name="loki", namespace="loki", type="Reconciling", status="True"}[10m]) > 0.1
        or
        increase(kube_helmrelease_status_condition{name="loki", namespace="loki", type="Reconciling", status="True"}[1h]) > 3
      for: 5m
      labels:
        severity: warning
        component: loki
        category: helmrelease
        team: sre
        service: observability
      annotations:
        summary: "‚ö†Ô∏è Loki experiencing frequent upgrades"
        description: |
          Loki HelmRelease is being upgraded more frequently than expected.
          
          **Current State:**
          - Upgrade rate: {{ $value | humanize }} upgrades/minute
          - Recent upgrades: {{ $value }} in the last hour
          
          **Common Causes:**
          - Configuration value changes being detected repeatedly
          - Secret value changes (MinIO credentials)
          - Resource constraint changes
          - Node selector changes
          - Storage configuration updates
          
          **Impact:**
          - Potential service disruption during upgrades
          - Increased resource usage
          - Log ingestion may be temporarily unavailable
          
          **Action Required:**
          1. Check HelmRelease status: `kubectl describe helmrelease loki -n loki`
          2. Review recent changes: `kubectl get events -n loki --sort-by='.lastTimestamp'`
          3. Check for configuration drift: Compare current values with desired state
          4. Monitor for pattern in upgrade triggers
          5. Consider stabilizing configuration values
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/loki/loki-frequent-upgrades.md"
        dashboard_url: "https://grafana.bruno.com/d/loki-overview"
    
    # üî¥ CRITICAL: Loki upgrade failures
    - alert: LokiUpgradeFailures
      expr: |
        kube_helmrelease_status_condition{name="loki", namespace="loki", type="Ready", status="False"} == 1
        and
        kube_helmrelease_status_condition{name="loki", namespace="loki", type="Ready", reason="Failed"} == 1
      for: 3m
      labels:
        severity: critical
        component: loki
        category: helmrelease
        team: sre
        service: observability
      annotations:
        summary: "üî¥ CRITICAL: Loki upgrade has failed"
        description: |
          Loki HelmRelease upgrade has failed and is in a failed state.
          
          **Error Details:**
          - Release: {{ $labels.name }}
          - Namespace: {{ $labels.namespace }}
          - Status: {{ $labels.status }}
          - Reason: {{ $labels.reason }}
          
          **Common Causes:**
          - Storage configuration issues (MinIO connectivity)
          - Resource constraints during upgrade
          - Configuration validation failures
          - Dependency issues
          
          **Impact:**
          - Log aggregation is unavailable
          - Observability stack is incomplete
          - Applications cannot send logs
          
          **Action Required:**
          1. Check HelmRelease status: `kubectl describe helmrelease loki -n loki`
          2. Check Loki pods: `kubectl get pods -n loki`
          3. Review upgrade logs: `kubectl logs -n flux-system -l app=flux`
          4. Check MinIO connectivity: `kubectl get pods -n loki -l app.kubernetes.io/name=minio`
          5. Verify storage configuration and credentials
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/loki/loki-upgrade-failed.md"
        dashboard_url: "https://grafana.bruno.com/d/loki-overview"
    
    # üü° WARNING: Loki configuration drift detected
    - alert: LokiConfigurationDrift
      expr: |
        kube_helmrelease_status_condition{name="loki", namespace="loki", type="Reconciling", status="True"} == 1
        and
        kube_helmrelease_status_condition{name="loki", namespace="loki", type="Ready", status="False"} == 1
      for: 15m
      labels:
        severity: warning
        component: loki
        category: configuration
        team: sre
        service: observability
      annotations:
        summary: "‚ö†Ô∏è Loki configuration drift detected"
        description: |
          Loki HelmRelease has been reconciling for more than 15 minutes, indicating potential configuration drift.
          
          **Possible Causes:**
          - Configuration values are changing repeatedly
          - Secret values are being updated frequently
          - Resource constraints are causing reconciliation loops
          - Node scheduling issues
          
          **Action:**
          1. Check for configuration changes: `kubectl get helmrelease loki -n loki -o yaml`
          2. Review recent events: `kubectl get events -n loki --sort-by='.lastTimestamp'`
          3. Check for resource constraints
          4. Monitor for pattern in reconciliation triggers
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/loki/loki-configuration-drift.md"

  # =============================================================================
  # üîß FLUX GITOPS MONITORING
  # =============================================================================
  - name: flux.gitops
    interval: 30s
    rules:
    # üî¥ CRITICAL: Flux controller issues
    - alert: FluxControllerIssues
      expr: |
        up{job="flux-controller", namespace="flux-system"} == 0
        or
        kube_pod_status_phase{namespace="flux-system", pod=~"flux-.*", phase!="Running"} == 1
      for: 2m
      labels:
        severity: critical
        component: flux
        category: gitops
        team: sre
        service: platform
      annotations:
        summary: "üî¥ CRITICAL: Flux controller issues detected"
        description: |
          Flux GitOps controller is not responding or pods are not running.
          
          **Impact:**
          - GitOps deployments may be stalled
          - Infrastructure components may not be updated
          - Manual intervention may be required
          
          **Action:**
          1. Check Flux pods: `kubectl get pods -n flux-system`
          2. Check Flux logs: `kubectl logs -n flux-system -l app=flux`
          3. Verify Git repository connectivity
          4. Check for resource constraints
        runbook_url: "https://github.com/brunolucena/homelab/blob/main/runbooks/flux/flux-controller-issues.md"
