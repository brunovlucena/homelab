# Default values for knative-lambda.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
#
# üö® CRITICAL FIXES APPLIED (2024-12-19):
# üö® 1. target: "0.01" ‚Üí "1" (FIXED: was creating 100x more pods than needed)
# üö® 2. containerConcurrency: "0" (KEPT: correct for unlimited concurrency per container)
# üö® 3. targetConcurrency: "1" (KEPT: reasonable for base configuration)
#
# üìä EXPECTED IMPACT:
# üìä - Pod count should reduce by ~90% for same traffic
# üìä - Cost reduction: ~90% fewer pods = ~90% lower resource usage
# üìä - Performance: Slightly higher latency but much better resource efficiency
# üìä - Stability: More predictable scaling behavior
#
# ‚ö†Ô∏è  MONITORING REQUIRED:
# ‚ö†Ô∏è  - Watch pod count after deployment
# ‚ö†Ô∏è  - Monitor response times for any degradation
# ‚ö†Ô∏è  - Check resource utilization metrics

# Application configuration
app:
  name: knative-lambda
  component: service
  partOf: knative-lambda-platform
  version: "1.0.0"  # Semantic version - updated via make version-bump

# Namespace configuration
namespace:
  create: false
  name: ""


# Knative broker configuration
broker:
  url: ""  # Will be auto-generated if not set: http://knative-lambda-service-broker-{environment}-broker-ingress.knative-lambda-{environment}.svc.cluster.local

# AWS Configuration
aws:
  accountId: "339954290315"
  region: "us-west-2"

# RabbitMQ Configuration
rabbitmq:
  clusterName: "rabbitmq-cluster"  # Will be overridden by environment-specific values
  namespace: "rabbitmq"  # Will be overridden by environment-specific values
  connectionSecretName: "rabbitmq-connection"
  # Eventing configuration
  eventing:
    parallelism: 50  # Number of parallel consumers for RabbitMQ events
  # Queue configuration
  queues:
    kanikoJobs:
      name: "kaniko-jobs"
      durable: true
      autoDelete: false
      messageTtl: 86400000  # 24 hours in milliseconds
      maxLength: 10000      # Maximum 10,000 messages
      overflow: "drop-head" # Drop oldest messages when full
  
# Environment configuration
roleName: "knative-lambda-builder" # TODO: Verify if this is default when removed. Optional

# Container image configuration
image:
  registry: 339954290315.dkr.ecr.us-west-2.amazonaws.com # TODO: USE BRUNOVLUCENA gh
  repository: knative-lambdas/knative-lambda-builder
  tag: "1.0.0"  # Semantic version - updated via make version-bump
  pullPolicy: Always

# Builder service configuration
builder:
  name: knative-lambda-builder
  minScale: 0
  maxScale: 10
  targetConcurrency: 5  # Set to 5 for proper parallelism
  scaleToZeroGracePeriod: "30s"
  scaleDownDelay: "0s"
  stableWindow: "10s"  # Reduced for faster scaling response
  
  # Resource configuration
  resources:
    requests:
      memory: "256Mi"
      cpu: "250m"
    limits:
      memory: "256Mi"
      cpu: "250m"
      
  # Timeout configuration
  timeouts:
    response: "300"
    idle: "60"

# Prometheus monitoring configuration
monitoring:
  enabled: true
  prometheus:
    scrape: "true"
    path: "/metrics"
    port: "http1"
  # Alerting configuration
  alerting:
    enabled: true
    # Alert thresholds
    thresholds:
      errorRate: 5.0  # Percentage
      latencyP95: 2.0  # Seconds
      buildFailureRate: 10.0  # Percentage
      cpuUsage: 80.0  # Percentage
      memoryUsage: 85.0  # Percentage
      queueDepth: 50  # Items
  # SLO/SLI configuration
  slos:
    enabled: true
    availability: 99.9  # Percentage
    latencyP95: 2.0  # Seconds
    errorRate: 0.1  # Percentage
    buildSuccessRate: 99.0  # Percentage
    buildDurationP95: 1800  # Seconds (30 minutes)
    queueDepth: 50  # Items
  # Dashboard configuration
  dashboards:
    enabled: true
    refresh: "30s"
    timeRange: "1h"
    metrics:
      latency: 5.0    # Seconds (95th percentile)
      cpuUsage: 80.0  # Percentage
      memoryUsage: 200 # MB
      queueDepth: 50  # Items
      buildFailureRate: 10.0 # Percentage
      goroutineCount: 1000
      fileDescriptorCount: 1000
      networkConnectionCount: 500
      diskSpaceUsage: 5 # GB
      dependencyLatency: 10.0 # Seconds
      requestSize: 10485760 # 10MB in bytes
      responseSize: 52428800 # 50MB in bytes
      buildCost: 100.0 # Dollars per hour
      userActivity: 100 # Activities per second

# K6 Load Testing configuration
k6Tests:
  enabled: false

# Lambda services configuration
# üö® UPDATED (2024-12-19): Refactored from dynamicServices to lambdaServices
# üí° Strategy: Configuration for dynamically created lambda services
lambdaServices:
  # Default configuration for generated lambda services
  defaults:
    # üîß AUTOSCALING: Configuration for lambda service autoscaling
    # üö® CRITICAL FIX: Updated for proper concurrency handling
    minScale: 0  # ‚úÖ KnativeMinScaleDefault = "0"
    maxScale: 50  # ‚úÖ KnativeMaxScaleDefault = "50"
    targetConcurrency: 10  # üö® FIXED: 10 request per pod for aggressive scaling
    targetUtilization: 50  # üö® FIXED: 50% of target concurrency triggers new pod
    target: 10  # üö® FIXED: Autoscaling target of 1 concurrent request
    containerConcurrency: 10  # üö® FIXED: 1 request per container for maximum distribution
    scaleToZeroGracePeriod: "30s"  # ‚úÖ KnativeScaleToZeroGracePeriodDefault = "30s"
    scaleDownDelay: "0s"  # ‚úÖ KnativeScaleDownDelayDefault = "0s"
    stableWindow: "10s"  # ‚úÖ KnativeStableWindowDefault = "10s"
    # üöÄ PANIC MODE: For rapid scaling during traffic spikes
    panicWindowPercentage: "10.0"  # 10% of stable window for panic mode
    panicThresholdPercentage: "200.0"  # 200% of target triggers panic mode
    
    # üì¶ RESOURCE CONFIGURATION - Specific to lambda services
    resources:
      requests:
        memory: "64Mi"
        cpu: "50m"
      limits:
        memory: "256Mi"
        cpu: "100m"
    
    # ‚è∞ TIMEOUT CONFIGURATION - Specific to lambda services
    timeouts:
      response: "60s"
      idle: "30s"
  
  # Template configuration for lambda services
  template:
    labels:
      app.kubernetes.io/managed-by: knative-lambda-builder
      app.kubernetes.io/part-of: knative-lambda-platform
    annotations:
      autoscaling.knative.dev/class: "kpa.autoscaling.knative.dev"

# HPA Configuration
hpa:
  enabled: true
  minReplicas: 3
  maxReplicas: 20
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

# Builder Service Configuration
builderService:
  targetConcurrency: 70  # 1 request per pod for aggressive scaling
  
  targetUtilization: 50  # 50% of target concurrency triggers new pod
  
  target: 70  # Aggressive scaling for lambda workloads
  
  containerConcurrency: 50
  
  # üîΩ Min Scale: Minimum number of pods (0 = scale to zero)
  minScale: "0"  # Can scale to zero when no traffic (cost savings)
  
  # üîº Max Scale: Maximum number of pods (safety limit)
  maxScale: "50"  # Max 50 pods (safety limit)
  
  # ‚è∞ Scale to Zero Grace Period: How long to wait before scaling to zero
  scaleToZeroGracePeriod: "30s"  # Wait 30s before scaling to zero
  
  # ‚è±Ô∏è Scale Down Delay: How long to wait before scaling down
  scaleDownDelay: "0s"  # Scale down immediately (base config)
  
  # üìä Stable Window: Time window for scaling decisions
  stableWindow: "10s"  # 10s window for scaling decisions (base config)

# Service account configuration
serviceAccount:
  create: true
  name: ""
  annotations: {}
  # Note: EKS Pod Identity is configured at the cluster level via pod identity associations
  # The role ARN is no longer specified in the service account annotation

# Environment variables - Base configuration (overridden by environment-specific overlays)
env:
  # Service Configuration
  httpPort: "8080"
  metricsPort: "8080"  # üîß FIXED: Metrics are served on the same HTTP port via /metrics endpoint
  logLevel: "info"
  
  # HTTP Configuration - Missing variables
  timeout: "30s"
  apiTimeout: "400ms"
  maxRequestSize: "10485760"
  defaultListLimit: "50"
  maxListLimit: "100"
  validateInput: "true"
  
  # Service Information - Missing variables
  serviceName: "knative-lambda-new"
  serviceVersion: "1.0.0"
  
  # AWS Configuration
  awsRegion: "us-west-2"
  awsAccountId: "339954290315"
  ecrBaseRegistry: "339954290315.dkr.ecr.us-west-2.amazonaws.com"
  ecrRepositoryName: "knative-lambdas"
  s3SourceBucket: "notifi-uw2-dev-fusion-modules"
  s3TmpBucket: "knative-lambda-dev-context-tmp"
  
  # EKS Pod Identity Configuration
  useEksPodIdentity: "true"
  podIdentityRole: "knative-lambda-builder"
  
  # Storage Configuration
  # üíæ STORAGE PROVIDER - Switch between AWS S3 and local MinIO
  # Options: "aws-s3" (default for production) or "minio" (local/homelab)
  storageProvider: "aws-s3"  # üîß Change to "minio" for homelab/local dev
  
  # S3 Configuration (used when storageProvider: "aws-s3")
  s3Endpoint: "https://s3.us-west-2.amazonaws.com"
  
  # MinIO Configuration (used when storageProvider: "minio")
  # üè† MinIO is S3-compatible object storage for on-premises/homelab deployments
  minioEndpoint: "minio.minio.svc.cluster.local:9000"  # MinIO service endpoint
  minioAccessKey: "minioadmin"  # MinIO access key (override via secret)
  minioSecretKey: "minioadmin"  # MinIO secret key (override via secret)
  minioUseSSL: "false"  # Use HTTP for internal cluster communication
  minioRegion: "us-east-1"  # MinIO default region
  minioSourceBucket: "knative-lambda-source"  # Source code bucket in MinIO
  minioTempBucket: "knative-lambda-tmp"  # Temporary build context bucket in MinIO
  
  # CloudEvents Broker Configuration
  brokerTopic: "parser-results"
  
  # RabbitMQ Configuration
  rabbitmqEventingParallelism: "50"  # Number of parallel consumers for RabbitMQ events
  
  # Observability Configuration - Missing variables
  metricsEnabled: "true"
  tracingEnabled: "true"
  otelExporterOtlpEndpoint: "tempo-distributor.tempo.svc.cluster.local:4317"
  sampleRate: "1.0"
  exemplarsEnabled: "true"
  exemplarsMaxPerMetric: "10"
  exemplarsSampleRate: "0.1"
  exemplarsTraceIDLabel: "trace_id"
  exemplarsSpanIDLabel: "span_id"
  exemplarsIncludeLabels: "third_party_id,parser_id,job_type,status,component,error_type"
  rateLimitingEnabled: "true"
  
  # Lambda Worker Pool Configuration
  lambdaWorkerPoolSize: "5"
  lambdaWorkerPoolCapacity: "10"
  lambdaEventQueueSize: "50"
  
  # Registry Configuration
  registryMirror: ""  # üîß Set to empty to pull base images from Docker Hub directly
  skipTlsVerifyRegistry: "docker.io"
  
  # Base Image Configuration
  nodeBaseImage: "docker.io/library/node:22-alpine"
  pythonBaseImage: "docker.io/library/python:3.11-alpine"
  goBaseImage: "docker.io/library/golang:1.21-alpine"
  
  # Notifi Service Addresses Configuration
  # These are the internal service addresses that lambda functions need to connect to
  subscriptionManagerAddress: "notifi-subscription-manager.notifi.svc.cluster.local:4000"
  ephemeralStorageAddress: "notifi-storage-manager.notifi.svc.cluster.local:4000"
  persistentStorageAddress: "notifi-storage-manager.notifi.svc.cluster.local:4000"
  fusionFetchProxyAddress: "notifi-fetch-proxy.notifi.svc.cluster.local:4000"
  evmRpcAddress: "notifi-blockchain-manager.notifi.svc.cluster.local:4000"
  solanaRpcAddress: "notifi-blockchain-manager.notifi.svc.cluster.local:4000"
  suiRpcAddress: "notifi-blockchain-manager.notifi.svc.cluster.local:4000"
  grpcInsecure: "true"
  
  # Rate Limiting Configuration
  buildContextRequestsPerMin: "5"
  buildContextBurstSize: "2"
  k8sJobRequestsPerMin: "10"
  k8sJobBurstSize: "3"
  clientRequestsPerMin: "5"
  clientBurstSize: "2"
  s3UploadRequestsPerMin: "50"
  s3UploadBurstSize: "10"
  maxMemoryUsagePercent: "80.0"
  memoryCheckInterval: "30s"
  cleanupInterval: "5m"
  clientTTL: "1h"
  maxConcurrentBuilds: "10"
  maxConcurrentJobs: "5"
  buildTimeout: "30m"
  jobTimeout: "1h"
  requestTimeout: "5m"
  jobTTLSeconds: "3600"
  
  # Scheduler Service Configuration
  schedulerUrl: "http://notifi-scheduler.notifi.svc.cluster.local/fusion/execution/response"
  
  # Encryption Configuration
  encryptionKey: "dGVmYXVsdC1lbmNyeXB0aW9uLWtleS1mb3ItZGVmYXVsdC1lbnZpcm9ubWVudA=="  # Base64 encoded 32-byte key
  
  # Kubernetes Configuration
  inCluster: "true"
  runAsUser: "1000"
  jobDeletionWaitTimeout: "30s"
  jobDeletionCheckInterval: "5s"
  
  # Lambda Function Configuration (fixed naming to match Go code)
  lambdaRuntime: "nodejs22"
  lambdaHandler: "index.handler"
  lambdaTrigger: "http"
  lambdaFunctionMemoryLimit: "256Mi"
  lambdaFunctionCpuLimit: "100m"
  lambdaFunctionMemoryRequest: "64Mi"
  lambdaFunctionCpuRequest: "50m"
  functionMemoryLimitMi: "256"
  functionCpuLimitM: "100"
  
  # Kaniko Configuration
  kanikoEnabled: "true"
  kanikoTimeout: "30m"
  kanikoCache: "true"
  kanikoCacheTtl: "24h"
  kanikoSkipTlsVerify: "true"
  kanikoInsecureRegistry: "true"
  kanikoDockerConfig: "/kaniko/.docker/config.json"
  kanikoImagePullSecret: "ecr-secret"
  kanikoBuildContextSizeLimit: "2Gi"
  kanikoBuildContextTimeout: "10m"
  kanikoDestination: "339954290315.dkr.ecr.us-west-2.amazonaws.com/knative-lambdas"
  kanikoRunAsUser: "1000"
  kanikoRunAsGroup: "1000"
  kanikoSecurityContextRunAsNonRoot: "true"
  kanikoSecurityContextReadOnlyRootFilesystem: "true"
  
  # Kaniko NPM Configuration
  kanikoNpmRegistry: "https://registry.npmjs.org/"
  kanikoNpmTimeout: "60000"
  kanikoNpmFetchRetries: "5"
  kanikoNpmFetchRetryMintimeout: "10000"
  kanikoNpmFetchRetryMaxtimeout: "60000"
  kanikoNpmFetchRetryFactor: "2"
  kanikoNpmPreferOffline: "true"
  kanikoNpmAudit: "false"
  kanikoNpmFund: "false"
  kanikoNpmUpdateNotifier: "false"
  kanikoNpmLoglevel: "warn"
  
  # Build Configuration
  kanikoImage: "gcr.io/kaniko-project/executor:v1.19.2"
  sidecarImage: ""
  cpuRequest: "500m"
  cpuLimit: "1000m"
  memoryRequest: "1Gi"
  memoryLimit: "2Gi"
  maxParserSize: "104857600"
  
  # Knative Configuration
  defaultEventType: "network.notifi.lambda.parser.start"
  brokerName: "knative-lambda-service-broker-dev"
  triggerNamespace: "knative-lambda-dev"
  defaultDeliveryRetries: "5"
  defaultDeliveryBackoffPolicy: "exponential"
  defaultDeliveryBackoffDelay: "PT1S"
  
  # RabbitMQ Configuration
  rabbitmqEventingParallelism: "50"
  
  # Security Configuration
  securityEnabled: "true"
  debugMode: "false"
  dryRun: "false"
  
  # EKS Pod Identity Configuration
  useEksPodIdentity: "true"
  podIdentityRole: "knative-lambda-builder"
  
# Node selector
nodeSelector:
  kubernetes.io/arch: arm64

# Tolerations
tolerations:
  - key: node.kubernetes.io/not-ready
    operator: Exists
    effect: NoExecute
    tolerationSeconds: 300
  - key: node.kubernetes.io/unreachable
    operator: Exists
    effect: NoExecute
    tolerationSeconds: 300

# Additional volumes
volumes:
  tmp:
    enabled: true
    emptyDir: {}
  cache:
    enabled: true
    emptyDir: {}

# Volume mounts
volumeMounts:
  tmp:
    mountPath: /tmp
  cache:
    mountPath: /var/cache

# Docker Hub configuration for Kaniko authentication
dockerHub:
  enabled: true
  username: ""  # Set via --set dockerHub.username=YOUR_USERNAME
  password: ""  # Set via --set dockerHub.password=YOUR_PASSWORD

# Sidecar configuration for build monitoring
sidecar:
  # Container image configuration
  image:
    registry: 339954290315.dkr.ecr.us-west-2.amazonaws.com
    repository: knative-lambdas/knative-lambda-sidecar
    tag: "1.0.0"  # Semantic version - updated via make version-bump
    pullPolicy: Always
  
  # Monitoring configuration
  monitoring:
    kanikoContainerName: "kaniko"
    pollInterval: "5s"
    buildTimeout: "30m"
  
  # Resource configuration
  resources:
    requests:
      memory: "128Mi"
      cpu: "100m"
    limits:
      memory: "128Mi"
      cpu: "100m"
  
  # Logging configuration
  logging:
    level: "info"
    format: "json"
  
  # Security configuration
  security:
    runAsUser: 1000
    runAsGroup: 1000
    runAsNonRoot: true
    readOnlyRootFilesystem: false
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL

# Metrics Pusher Configuration
metricsPusher:
  enabled: true  # üöÄ ACTIVATED: Enable metrics pusher for enhanced monitoring
  # Container image configuration
  image:
    registry: 339954290315.dkr.ecr.us-west-2.amazonaws.com
    repository: knative-lambdas/knative-lambda-metrics-pusher
    tag: "1.0.0"  # Semantic version - updated via make version-bump
    pullPolicy: Always
  
  # Resource configuration
  resources:
    requests:
      memory: "64Mi"
      cpu: "50m"
    limits:
      memory: "128Mi"
      cpu: "100m"
  
  # Configuration
  config:
    pushInterval: "30s"
    timeout: "10s"
    logLevel: "info"

# Rate Limiting Configuration
rateLimiting:
  enabled: true
  
  # Build Context Rate Limiting
  buildContext:
    requestsPerMin: 5
    burstSize: 2
  
  # Kubernetes Job Rate Limiting
  k8sJob:
    requestsPerMin: 10
    burstSize: 3
  
  # Per-Client Rate Limiting
  client:
    requestsPerMin: 5
    burstSize: 2
  
  # S3 Upload Rate Limiting
  s3Upload:
    requestsPerMin: 50
    burstSize: 10
  
  # Memory Management
  memory:
    maxUsagePercent: 80.0
    checkInterval: "30s"
  
  # Cleanup Configuration
  cleanup:
    interval: "5m"
    clientTTL: "1h"

# Job Management Configuration
jobManagement:
  enabled: true  # üöÄ ENABLED: Job management features
  # Maximum number of concurrent jobs
  maxConcurrentJobs: 20
  
  # Job timeout configuration
  jobTimeout: "1h"
  
  # Job cleanup configuration
  cleanup:
    enabled: true
    schedule: "0 2 * * *"  # Daily at 2 AM (cron format)
    ttlSecondsAfterFinished: 3600  # 1 hour
    maxJobsToKeep: 10
    maxFailedJobsToKeep: 5
    
  # Job deduplication configuration
  deduplication:
    enabled: true
    checkExistingJobs: true
    skipIfJobExists: true
    jobNamePrefix: "build-"
    
  # Job lifecycle management
  lifecycle:
    # Automatically clean up completed jobs
    autoCleanup: true
    cleanupInterval: "5m"
    cleanupBatchSize: 50
    
    # Job retention policies
    retention:
      successfulJobs: "1h"    # Keep successful jobs for 1 hour
      failedJobs: "24h"       # Keep failed jobs for 24 hours
      pendingJobs: "30m"      # Keep pending jobs for 30 minutes
  
# Security Configuration
security:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000

# Environment Variables (additional)
additionalEnv: []
