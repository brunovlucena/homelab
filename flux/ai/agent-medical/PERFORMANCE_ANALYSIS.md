# Performance Analysis: Lambda Tool Calling in gRPC Chat

**Date**: January 2025  
**Context**: Medical agent with gRPC chat interface using Lambda functions for tool calling  
**SRE Perspective**: Bottleneck analysis and optimization strategies

---

## üö® Potential Bottlenecks

### 1. **Cold Start Latency** ‚ö†Ô∏è HIGH IMPACT

**Current Configuration**:
- Scale-to-zero grace period: `300s` (5 minutes)
- Min replicas: `0`
- Cold start time: ~5-10s (Knative Lambda)

**Impact on Chat UX**:
```
User sends message ‚Üí gRPC request ‚Üí Agent processes ‚Üí Tool call needed
‚Üí Lambda cold start (5-10s) ‚Üí Tool execution ‚Üí Response
Total: 5-15s delay (unacceptable for chat)
```

**User Experience**:
- Chat appears "frozen" for 5-15 seconds
- User may retry, causing duplicate requests
- Poor perceived performance

---

### 2. **Synchronous Blocking** ‚ö†Ô∏è CRITICAL

**Current Flow** (if not async):
```
gRPC Request ‚Üí Agent ‚Üí Wait for Lambda ‚Üí Wait for Response ‚Üí gRPC Response
```

**Problem**: If tool calling blocks the gRPC response, the entire chat is blocked.

**Impact**:
- Chat timeout (120s in iOS app)
- User sees "Processing..." indefinitely
- Connection may drop
- Poor UX

---

### 3. **Network Latency** ‚ö†Ô∏è MEDIUM IMPACT

**CloudEvents Path**:
```
Mobile App ‚Üí Mobile API ‚Üí RabbitMQ Broker ‚Üí Knative Trigger ‚Üí Lambda
‚Üí Processing ‚Üí Response Event ‚Üí RabbitMQ ‚Üí Mobile API ‚Üí gRPC Stream ‚Üí App
```

**Latency Breakdown**:
- RabbitMQ hop: 50-150ms
- Network overhead: 100-300ms
- Total added latency: 150-450ms per tool call

---

### 4. **Concurrency Limits** ‚ö†Ô∏è MEDIUM IMPACT

**Current Limits**:
- Max replicas: `10`
- Target concurrency: `5` per replica
- Max concurrent requests: `50`

**Impact**:
- Under high load, requests queue
- Tool calls may timeout
- Degraded performance during peak usage

---

## ‚úÖ Solutions & Recommendations

### Solution 1: **Async Tool Calling with Callbacks** üéØ CRITICAL

**Architecture Pattern**: Fire-and-forget with gRPC streaming response

```python
# agent-medical/main.py
async def handle_grpc_chat(request):
    """Handle gRPC chat request - non-blocking"""
    
    # 1. Generate immediate acknowledgment
    conversation_id = request.conversation_id
    
    # 2. Send initial response (immediate)
    yield ChatResponse(
        message="Processing your request...",
        status="processing",
        conversation_id=conversation_id
    )
    
    # 3. Trigger tool call asynchronously (non-blocking)
    asyncio.create_task(process_tool_call_async(
        tool_name=request.tool_name,
        params=request.params,
        conversation_id=conversation_id
    ))
    
    # 4. Return immediately - don't wait for tool
    return  # gRPC stream continues
```

**Tool Call Handler** (async):
```python
async def process_tool_call_async(tool_name, params, conversation_id):
    """Process tool call and emit response event"""
    
    # Emit CloudEvent to Lambda (async)
    event = CloudEvent({
        "type": f"io.homelab.medical.tool.{tool_name}",
        "source": "/agent-medical/chat",
        "subject": conversation_id,
    }, params)
    
    # Send to broker (non-blocking)
    await send_to_broker(event)
    
    # Lambda processes and emits response event
    # Response comes back via gRPC stream (see Solution 2)
```

**Benefits**:
- ‚úÖ Chat responds immediately (<100ms)
- ‚úÖ No blocking on tool calls
- ‚úÖ Better UX (shows "Processing..." state)
- ‚úÖ Scalable (handles multiple concurrent tool calls)

---

### Solution 2: **gRPC Streaming for Responses** üéØ RECOMMENDED

**Implementation**: Use bidirectional gRPC streaming

```protobuf
// medical_chat.proto
service MedicalChat {
  rpc Chat(stream ChatRequest) returns (stream ChatResponse);
}

message ChatRequest {
  string message = 1;
  string conversation_id = 2;
  string patient_id = 3;
  optional ToolCall tool_call = 4;
}

message ChatResponse {
  string message = 1;
  string conversation_id = 2;
  ResponseStatus status = 3;
  optional ToolResult tool_result = 4;
}
```

**Mobile API Gateway** (Go):
```go
// mobile-api/grpc-handler.go
func (s *MobileAPIServer) Chat(stream pb.MedicalChat_ChatServer) error {
    // Maintain connection
    for {
        // Receive chat request
        req, err := stream.Recv()
        if err != nil {
            return err
        }
        
        // Send to agent (async)
        go func() {
            // Process via agent-medical
            response := processChatRequest(req)
            
            // Stream response back
            stream.Send(&pb.ChatResponse{
                Message: response.Message,
                ConversationId: req.ConversationId,
                Status: pb.ResponseStatus_SUCCESS,
            })
        }()
    }
}

// Subscribe to CloudEvents for tool responses
func (s *MobileAPIServer) subscribeToToolResponses() {
    // Listen for io.homelab.medical.tool.response events
    // Map conversation_id to active gRPC streams
    // Forward response via gRPC stream
}
```

**Benefits**:
- ‚úÖ Real-time updates
- ‚úÖ Low latency (HTTP/2 multiplexing)
- ‚úÖ Efficient (single connection)
- ‚úÖ Type safety (Protobuf)

---

### Solution 3: **Pre-warming for Critical Functions** üéØ HIGH PRIORITY

**Strategy**: Keep critical Lambda functions warm

**Option A: Keep Minimum Replicas** (for hot paths)
```yaml
# lambdaagent.yaml
scaling:
  minReplicas: 1  # Keep 1 replica warm for heart-rate-analyzer
  maxReplicas: 10
  targetConcurrency: 5
  scaleToZeroGracePeriod: 60s  # Reduced from 300s
```

**Option B: Scheduled Pre-warming**
```yaml
# k8s/cronjob-pre-warm.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: pre-warm-heart-rate-analyzer
spec:
  schedule: "*/5 * * * *"  # Every 5 minutes
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: pre-warm
            image: curlimages/curl
            command:
            - /bin/sh
            - -c
            - |
              curl -X POST http://agent-medical.ai.svc.cluster.local:8080/analyze/heart-rate \
                -H "Content-Type: application/json" \
                -d '{"patient_id": "pre-warm", "heart_rate_bpm": 70, "context": "resting"}'
```

**Option C: Keep-Alive Endpoint**
```python
# agent-medical/main.py
@app.get("/keep-alive")
async def keep_alive():
    """Keep Lambda warm - called by health checks"""
    return {"status": "warm", "timestamp": datetime.utcnow()}
```

**Recommendation**: Use **Option A** for `heart-rate-analyzer` (critical path) + **Option C** for general keep-alive.

---

### Solution 4: **Reduce Scale-to-Zero Grace Period** üéØ MEDIUM PRIORITY

**Current**: `300s` (5 minutes)  
**Recommended**: `60s` (1 minute) for chat-adjacent functions

```yaml
# lambdaagent.yaml
scaling:
  minReplicas: 0
  maxReplicas: 10
  targetConcurrency: 5
  scaleToZeroGracePeriod: 60s  # Reduced for faster scaling
```

**Trade-off**:
- ‚úÖ Faster cold starts (1 min vs 5 min)
- ‚ö†Ô∏è Slightly higher resource usage
- ‚úÖ Better for chat UX

---

### Solution 5: **Connection Pooling & Caching** üéØ MEDIUM PRIORITY

**Connection Pooling**:
```python
# agent-medical/main.py
import aiohttp

# Reuse HTTP connections
session = aiohttp.ClientSession(
    connector=aiohttp.TCPConnector(limit=100),
    timeout=aiohttp.ClientTimeout(total=30)
)

async def call_lambda_tool(tool_name, params):
    """Call Lambda with connection pooling"""
    async with session.post(
        f"http://{tool_name}.ai.svc.cluster.local:8080/",
        json=params
    ) as response:
        return await response.json()
```

**Caching** (for read-only tool calls):
```python
from functools import lru_cache
import redis

redis_client = redis.Redis(host='redis.redis.svc.cluster.local')

async def get_patient_records(patient_id: str):
    """Cache patient records for 5 minutes"""
    cache_key = f"patient:{patient_id}:records"
    
    # Check cache
    cached = await redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Fetch from Lambda
    records = await call_lambda_tool("patient-records", {"patient_id": patient_id})
    
    # Cache for 5 minutes
    await redis_client.setex(cache_key, 300, json.dumps(records))
    
    return records
```

**Benefits**:
- ‚úÖ Reduced latency (cached responses)
- ‚úÖ Lower Lambda invocations
- ‚úÖ Better performance under load

---

### Solution 6: **Timeout Configuration** üéØ HIGH PRIORITY

**Current iOS Timeout**: `120s` (too long for chat)

**Recommended**:
```swift
// iOS App
request.timeoutInterval = 30.0  // 30 seconds for initial response
```

**gRPC Streaming** (no timeout for stream):
```swift
// Keep stream alive, no timeout
grpcStream.connect()  // Persistent connection
```

**Lambda Timeout**:
```yaml
# lambdaagent.yaml
resources:
  requests:
    cpu: "250m"
    memory: "512Mi"
  limits:
    cpu: "1000m"
    memory: "2Gi"
  timeout: 30s  # Add timeout for tool calls
```

---

## üìä Performance Targets

### Chat Response Times

| Metric | Target | Current (Blocking) | With Solutions |
|--------|--------|-------------------|----------------|
| Initial response | < 200ms | 5-15s (cold start) | < 100ms ‚úÖ |
| Tool call latency | < 2s | 5-15s | < 1s ‚úÖ |
| End-to-end (chat) | < 3s | 10-20s | < 2s ‚úÖ |
| Tool call (warm) | < 500ms | 1-2s | < 300ms ‚úÖ |

### Scalability

| Metric | Target | Current |
|--------|--------|---------|
| Concurrent users | 100+ | 50 (limited) |
| Tool calls/sec | 50+ | 20 (limited) |
| Cold start p99 | < 2s | 5-10s |
| Warm latency p99 | < 500ms | 1-2s |

---

## üèóÔ∏è Recommended Architecture

### Pattern: **Async Tool Calling with gRPC Streaming**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    üì± iOS Mobile App                           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  gRPC Stream (bidirectional)                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ 1. Send chat message                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ 2. Receive immediate acknowledgment (< 100ms)            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ 3. Receive tool result via stream (async)                ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îÇ gRPC (HTTP/2)
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Mobile API Gateway (Go)                            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Maintains gRPC stream connections                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Routes chat to agent-medical                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Subscribes to tool response events                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Forwards responses via gRPC stream                    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îÇ HTTP/CloudEvents
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Agent-Medical (Knative Service)                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ 1. Process chat (immediate response)                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ 2. If tool needed: emit CloudEvent (async)                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ 3. Return chat response immediately                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ 4. Tool result comes back via event                      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îÇ CloudEvent (async)
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         RabbitMQ Broker ‚Üí Knative Trigger                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îÇ CloudEvent
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Heart Rate Analyzer (Lambda - Pre-warmed)                 ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Process tool call                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Emit response event:                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   io.homelab.medical.tool.heart-rate.response          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îÇ CloudEvent (async)
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Mobile API Gateway (receives event)                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Maps conversation_id to gRPC stream                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Sends result via gRPC stream                           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îÇ gRPC stream
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    üì± iOS Mobile App                           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚Ä¢ Receives tool result via stream                             ‚îÇ
‚îÇ  ‚Ä¢ Updates UI asynchronously                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ Implementation Priority

### Phase 1: Critical (Week 1) üö®
1. ‚úÖ **Implement async tool calling** (don't block gRPC response)
2. ‚úÖ **Reduce iOS timeout** to 30s
3. ‚úÖ **Add immediate acknowledgment** in chat response

### Phase 2: High Priority (Week 2) üéØ
4. ‚úÖ **Implement gRPC streaming** for responses
5. ‚úÖ **Pre-warm heart-rate-analyzer** (minReplicas: 1)
6. ‚úÖ **Reduce scale-to-zero grace period** to 60s

### Phase 3: Optimization (Week 3) ‚ö°
7. ‚úÖ **Add connection pooling** for Lambda calls
8. ‚úÖ **Implement caching** for read-only tool calls
9. ‚úÖ **Add monitoring** for cold start metrics

---

## üìà Monitoring & Metrics

### Key Metrics to Track

```prometheus
# Cold start latency
agent_medical_lambda_cold_start_duration_seconds{function="heart-rate-analyzer"}

# Tool call latency
agent_medical_tool_call_duration_seconds{tool="heart-rate-analyzer", status="success"}

# gRPC response time
agent_medical_grpc_response_time_ms{status="success"}

# Chat end-to-end latency
agent_medical_chat_e2e_latency_seconds{conversation_id}

# Lambda warm/cold ratio
agent_medical_lambda_warm_ratio{function="heart-rate-analyzer"}
```

### Alerts

```yaml
# Alert on high cold start rate
- alert: HighColdStartRate
  expr: rate(agent_medical_lambda_cold_start_total[5m]) > 0.1
  for: 5m
  annotations:
    summary: "High cold start rate detected"

# Alert on slow tool calls
- alert: SlowToolCalls
  expr: histogram_quantile(0.99, agent_medical_tool_call_duration_seconds) > 2
  for: 5m
  annotations:
    summary: "Tool calls exceeding 2s p99 latency"
```

---

## ‚úÖ Summary

**Answer**: Yes, Lambda function calling **can be a bottleneck** if not designed properly, but it's **avoidable** with the right architecture.

**Key Solutions**:
1. ‚úÖ **Async tool calling** - Don't block gRPC response
2. ‚úÖ **gRPC streaming** - Real-time updates
3. ‚úÖ **Pre-warming** - Keep critical functions warm
4. ‚úÖ **Connection pooling** - Reduce latency
5. ‚úÖ **Caching** - Avoid redundant calls

**Result**: Chat UX remains responsive (< 200ms initial response) while tool calls happen asynchronously in the background.

