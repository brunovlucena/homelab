#!/usr/bin/env python3
"""
Fine-Tuning Script for FunctionGemma - Phase 4

Fine-tunes FunctionGemma 270M on SRE remediation dataset using MLX-LM framework.
"""
import argparse
import json
import sys
from pathlib import Path
from typing import List, Dict, Any
import structlog

logger = structlog.get_logger()


def load_training_data(data_path: str) -> List[Dict[str, Any]]:
    """Load training data from JSONL file."""
    examples = []
    
    with open(data_path, "r") as f:
        for line in f:
            line = line.strip()
            if line:
                examples.append(json.loads(line))
    
    logger.info("training_data_loaded", count=len(examples), path=data_path)
    return examples


def convert_to_mlx_format(examples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Convert training examples to MLX-LM format."""
    mlx_examples = []
    
    for ex in examples:
        prompt = ex["prompt"]
        completion = ex["completion"]
        
        # MLX-LM format: instruction + response
        mlx_examples.append({
            "instruction": prompt,
            "response": completion
        })
    
    logger.info("converted_to_mlx_format", count=len(mlx_examples))
    return mlx_examples


def create_mlx_training_script(
    model_name: str,
    data_path: str,
    output_path: str,
    epochs: int = 3,
    batch_size: int = 4,
    learning_rate: float = 1e-5
) -> str:
    """Create MLX-LM fine-tuning script."""
    script = f"""#!/usr/bin/env python3
\"\"\"
Fine-tune FunctionGemma 270M using MLX-LM.

Generated by fine_tune_functiongemma.py
\"\"\"
import json
from pathlib import Path
from mlx_lm import train, load

# Load base model
print("Loading base model: {model_name}")
model, tokenizer = load("{model_name}")

# Load training data
print("Loading training data: {data_path}")
with open("{data_path}", "r") as f:
    train_data = [json.loads(line) for line in f if line.strip()]

# Convert to MLX format
train_examples = []
for ex in train_data:
    train_examples.append({{
        "instruction": ex["prompt"],
        "response": ex["completion"]
    }})

# Save training data in MLX format
mlx_data_path = "{data_path}.mlx.json"
with open(mlx_data_path, "w") as f:
    json.dump(train_examples, f, indent=2)

print(f"Training on {{len(train_examples)}} examples...")
print(f"Epochs: {epochs}, Batch size: {batch_size}, Learning rate: {learning_rate}")

# Fine-tune
train(
    model=model,
    tokenizer=tokenizer,
    data=mlx_data_path,
    output_dir="{output_path}",
    num_epochs={epochs},
    batch_size={batch_size},
    learning_rate={learning_rate},
    val_split=0.1,  # 10% validation split
    save_every=1,  # Save after each epoch
)

print(f"‚úÖ Fine-tuning complete! Model saved to: {output_path}")
"""
    return script


def main():
    parser = argparse.ArgumentParser(description="Fine-tune FunctionGemma 270M for Agent-SRE")
    parser.add_argument(
        "--model",
        default="mlx-community/functiongemma-270m-it-bf16",
        help="Base model name (MLX format)"
    )
    parser.add_argument(
        "--data",
        required=True,
        help="Training data JSONL file path"
    )
    parser.add_argument(
        "--output",
        required=True,
        help="Output directory for fine-tuned model"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=3,
        help="Number of training epochs"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=4,
        help="Training batch size"
    )
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=1e-5,
        help="Learning rate"
    )
    parser.add_argument(
        "--generate-script-only",
        action="store_true",
        help="Only generate training script, don't run it"
    )
    
    args = parser.parse_args()
    
    # Validate inputs
    data_path = Path(args.data)
    if not data_path.exists():
        print(f"‚ùå Training data file not found: {data_path}")
        sys.exit(1)
    
    output_path = Path(args.output)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Load training data
    examples = load_training_data(str(data_path))
    
    if len(examples) == 0:
        print("‚ùå No training examples found!")
        sys.exit(1)
    
    print(f"üìä Loaded {len(examples)} training examples")
    
    # Generate training script
    script_content = create_mlx_training_script(
        model_name=args.model,
        data_path=str(data_path),
        output_path=str(output_path),
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate
    )
    
    script_path = output_path / "train.py"
    with open(script_path, "w") as f:
        f.write(script_content)
    
    script_path.chmod(0o755)  # Make executable
    
    print(f"üìù Generated training script: {script_path}")
    
    if args.generate_script_only:
        print("‚úÖ Script generation complete. Run it manually when ready.")
        return
    
    # Run training
    print("\nüöÄ Starting fine-tuning...")
    print("‚ö†Ô∏è  Note: Fine-tuning requires MLX-LM and can take significant time/resources")
    print(f"   Run: python {script_path}")
    
    # Optionally, we could run it here, but it's better to let user run it
    # import subprocess
    # subprocess.run(["python", str(script_path)])


if __name__ == "__main__":
    main()

