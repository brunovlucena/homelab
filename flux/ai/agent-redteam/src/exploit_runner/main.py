"""
Agent-Redteam - FastAPI entry point.

Security testing agent for running exploits against Knative Lambda Operator.

âš ï¸ WARNING: This agent is for AUTHORIZED TESTING ONLY.
Only run against test clusters you own or have explicit permission to test.

CloudEvents Integration:
- Emits events for exploit results (exploit.executed, exploit.success, exploit.blocked)
- Can receive events to trigger test runs (redteam.run)
"""
import os
import random
from contextlib import asynccontextmanager
from typing import Optional, Dict, Any

from fastapi import FastAPI, HTTPException, Query, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response, JSONResponse
from pydantic import BaseModel, Field
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
import structlog
import json
from opentelemetry import trace

from .handler import ExploitRunner
from shared.metrics import (
    init_build_info,
    init_catalog_gauges,
    AGENT_READY,
    DRY_RUN_MODE,
    HTTP_REQUESTS,
    HTTP_REQUEST_DURATION,
    HTTP_ERRORS,
    record_cloudevent,
)
from shared.types import ExploitSeverity, ExploitCategory

logger = structlog.get_logger()

# Global runner instance
runner: ExploitRunner = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize and cleanup resources."""
    global runner
    
    # Initialize exploit runner
    runner = ExploitRunner()
    
    # Initialize build info (also sets AGENT_UP)
    version = os.getenv("VERSION", "0.1.0")
    commit = os.getenv("GIT_COMMIT", "unknown")
    init_build_info(version, commit)
    
    # Initialize agent status gauges
    AGENT_READY.set(1)
    DRY_RUN_MODE.set(1 if runner.dry_run else 0)
    
    # Initialize catalog size gauges
    init_catalog_gauges(runner.catalog.exploits)
    
    logger.info(
        "agent_redteam_initialized",
        version=version,
        catalog_size=len(runner.catalog.exploits),
        dry_run=runner.dry_run,
    )
    
    yield
    
    # Cleanup
    AGENT_READY.set(0)
    logger.info("agent_redteam_shutdown")


app = FastAPI(
    title="Agent-Redteam",
    description="Security testing agent for running exploits against Knative Lambda Operator. âš ï¸ AUTHORIZED TESTING ONLY",
    version="0.1.0",
    lifespan=lifespan,
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=os.getenv("CORS_ORIGINS", "*").split(","),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# =============================================================================
# HTTP Request Tracking Middleware
# =============================================================================

@app.middleware("http")
async def track_http_requests(request: Request, call_next):
    """Track HTTP requests for metrics."""
    import time
    start_time = time.monotonic()
    method = request.method
    endpoint = request.url.path
    
    try:
        response = await call_next(request)
        status_code = response.status_code
        duration = time.monotonic() - start_time
        
        # Record metrics
        HTTP_REQUESTS.labels(method=method, endpoint=endpoint, status_code=str(status_code)).inc()
        HTTP_REQUEST_DURATION.labels(method=method, endpoint=endpoint).observe(duration)
        
        # Track errors
        if status_code >= 400:
            error_type = "client_error" if status_code < 500 else "server_error"
            HTTP_ERRORS.labels(method=method, endpoint=endpoint, error_type=error_type).inc()
        
        return response
    except Exception as e:
        duration = time.monotonic() - start_time
        error_type = type(e).__name__
        
        # Record error metrics
        HTTP_REQUESTS.labels(method=method, endpoint=endpoint, status_code="500").inc()
        HTTP_REQUEST_DURATION.labels(method=method, endpoint=endpoint).observe(duration)
        HTTP_ERRORS.labels(method=method, endpoint=endpoint, error_type=error_type).inc()
        
        raise


# =============================================================================
# Request/Response Models
# =============================================================================

class RunExploitRequest(BaseModel):
    """Request to run a single exploit."""
    exploit_id: str = Field(..., description="ID of the exploit to run (e.g., 'vuln-001')")
    manifest_name: Optional[str] = Field(None, description="Specific manifest file to run")
    namespace: Optional[str] = Field(None, description="Target namespace (defaults to configured)")


class RunTestRequest(BaseModel):
    """Request to run a test suite."""
    name: str = Field(..., description="Name for this test run")
    exploit_ids: Optional[list[str]] = Field(None, description="Specific exploit IDs to run")
    categories: Optional[list[str]] = Field(None, description="Run exploits from these categories")
    severities: Optional[list[str]] = Field(None, description="Run exploits of these severities")
    namespace: Optional[str] = Field(None, description="Target namespace")


class ExploitResultResponse(BaseModel):
    """Response for exploit execution."""
    exploit_id: str
    status: str
    started_at: str
    completed_at: str
    duration_seconds: float
    output: str = ""
    error: Optional[str] = None
    artifacts: list[str] = []
    mitigated_by: Optional[str] = None


class TestRunResponse(BaseModel):
    """Response for test run."""
    id: str
    name: str
    target_cluster: str
    target_namespace: str
    started_at: str
    completed_at: str
    status: str
    total_exploits: int
    successful_exploits: int
    blocked_exploits: int
    failed_exploits: int
    results: list[ExploitResultResponse]


class CatalogResponse(BaseModel):
    """Response for exploit catalog."""
    version: str
    exploits: list[dict]
    total: int


class HealthResponse(BaseModel):
    """Health check response."""
    status: str
    exploits_loaded: int = 0
    dry_run_mode: bool = False


# =============================================================================
# Endpoints
# =============================================================================

@app.get("/health", response_model=HealthResponse)
async def health():
    """Health check endpoint."""
    return HealthResponse(
        status="healthy" if runner else "degraded",
        exploits_loaded=len(runner.catalog.exploits) if runner else 0,
        dry_run_mode=runner.dry_run if runner else False,
    )


@app.get("/ready")
async def ready():
    """Readiness check endpoint."""
    if runner is None:
        raise HTTPException(status_code=503, detail="Runner not initialized")
    return {"status": "ready", "catalog_size": len(runner.catalog.exploits)}


@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint."""
    return Response(
        content=generate_latest(),
        media_type=CONTENT_TYPE_LATEST
    )


@app.get("/")
async def root():
    """Root endpoint with service info."""
    return {
        "service": "agent-redteam",
        "description": "Security testing agent for Knative Lambda Operator exploits",
        "version": os.getenv("VERSION", "0.1.0"),
        "warning": "âš ï¸ AUTHORIZED TESTING ONLY",
        "endpoints": {
            "catalog": "GET /catalog",
            "run_exploit": "POST /exploit/run",
            "run_test": "POST /test/run",
            "cleanup": "POST /cleanup",
            "health": "GET /health",
            "ready": "GET /ready",
            "metrics": "GET /metrics",
        }
    }


# =============================================================================
# Catalog Endpoints
# =============================================================================

@app.get("/catalog", response_model=CatalogResponse)
async def get_catalog(
    category: Optional[str] = Query(None, description="Filter by category"),
    severity: Optional[str] = Query(None, description="Filter by severity"),
):
    """
    Get the exploit catalog.
    
    Returns all available exploits, optionally filtered by category or severity.
    """
    if runner is None:
        raise HTTPException(status_code=503, detail="Runner not initialized")
    
    exploits = runner.catalog.exploits
    
    if category:
        try:
            cat = ExploitCategory(category)
            exploits = [e for e in exploits if e.category == cat]
        except ValueError:
            raise HTTPException(status_code=400, detail=f"Invalid category: {category}")
    
    if severity:
        try:
            sev = ExploitSeverity(severity)
            exploits = [e for e in exploits if e.severity == sev]
        except ValueError:
            raise HTTPException(status_code=400, detail=f"Invalid severity: {severity}")
    
    return CatalogResponse(
        version=runner.catalog.version,
        exploits=[e.to_dict() for e in exploits],
        total=len(exploits),
    )


@app.get("/catalog/{exploit_id}")
async def get_exploit(exploit_id: str):
    """Get details for a specific exploit."""
    if runner is None:
        raise HTTPException(status_code=503, detail="Runner not initialized")
    
    exploit = runner.catalog.get_by_id(exploit_id)
    if not exploit:
        raise HTTPException(status_code=404, detail=f"Exploit '{exploit_id}' not found")
    
    return exploit.to_dict()


# =============================================================================
# Exploit Execution Endpoints
# =============================================================================

@app.post("/exploit/run", response_model=ExploitResultResponse)
async def run_exploit(request: RunExploitRequest):
    """
    Run a single exploit.
    
    âš ï¸ WARNING: Only run against authorized test environments!
    
    This executes the specified exploit against the target cluster and
    returns the result including whether it succeeded or was mitigated.
    """
    if runner is None:
        raise HTTPException(status_code=503, detail="Runner not initialized")
    
    logger.info(
        "exploit_run_requested",
        exploit_id=request.exploit_id,
        namespace=request.namespace,
        source="api",
    )
    
    # Use shared business logic
    result = await runner.run_exploit(
        exploit_id=request.exploit_id,
        manifest_name=request.manifest_name,
        namespace=request.namespace,
    )
    
    return ExploitResultResponse(
        exploit_id=result.exploit_id,
        status=result.status.value,
        started_at=result.started_at,
        completed_at=result.completed_at,
        duration_seconds=result.duration_seconds,
        output=result.output,
        error=result.error,
        artifacts=result.artifacts,
        mitigated_by=result.mitigated_by,
    )


@app.post("/test/run", response_model=TestRunResponse)
async def run_test(request: RunTestRequest, background_tasks: BackgroundTasks):
    """
    Run a test suite of multiple exploits.
    
    âš ï¸ WARNING: Only run against authorized test environments!
    
    This executes multiple exploits and returns aggregated results.
    Can filter by exploit IDs, categories, or severities.
    """
    if runner is None:
        raise HTTPException(status_code=503, detail="Runner not initialized")
    
    # Convert string categories/severities to enums
    categories = None
    if request.categories:
        try:
            categories = [ExploitCategory(c) for c in request.categories]
        except ValueError as e:
            raise HTTPException(status_code=400, detail=f"Invalid category: {e}")
    
    severities = None
    if request.severities:
        try:
            severities = [ExploitSeverity(s) for s in request.severities]
        except ValueError as e:
            raise HTTPException(status_code=400, detail=f"Invalid severity: {e}")
    
    logger.info(
        "test_run_requested",
        name=request.name,
        exploit_ids=request.exploit_ids,
        categories=request.categories,
        severities=request.severities,
    )
    
    test_run = await runner.run_test(
        name=request.name,
        exploit_ids=request.exploit_ids,
        categories=categories,
        severities=severities,
        namespace=request.namespace,
    )
    
    return TestRunResponse(
        id=test_run.id,
        name=test_run.name,
        target_cluster=test_run.target_cluster,
        target_namespace=test_run.target_namespace,
        started_at=test_run.started_at,
        completed_at=test_run.completed_at,
        status=test_run.status,
        total_exploits=test_run.total_exploits,
        successful_exploits=test_run.successful_exploits,
        blocked_exploits=test_run.blocked_exploits,
        failed_exploits=test_run.failed_exploits,
        results=[
            ExploitResultResponse(
                exploit_id=r.exploit_id,
                status=r.status.value,
                started_at=r.started_at,
                completed_at=r.completed_at,
                duration_seconds=r.duration_seconds,
                output=r.output,
                error=r.error,
                artifacts=r.artifacts,
                mitigated_by=r.mitigated_by,
            )
            for r in test_run.results
        ],
    )


@app.post("/test/run-all")
async def run_all_exploits(
    namespace: Optional[str] = Query(None, description="Target namespace"),
):
    """
    Run ALL exploits in the catalog.
    
    âš ï¸ WARNING: This runs ALL exploits! Only use in test environments!
    """
    if runner is None:
        raise HTTPException(status_code=503, detail="Runner not initialized")
    
    test_run = await runner.run_test(
        name="full-suite",
        namespace=namespace,
    )
    
    return test_run.to_dict()


# =============================================================================
# Cleanup Endpoints
# =============================================================================

@app.post("/cleanup")
async def cleanup(namespace: Optional[str] = Query(None, description="Namespace to clean")):
    """
    Clean up all exploit resources.
    
    Removes all LambdaFunction resources with the redteam=true label.
    """
    if runner is None:
        raise HTTPException(status_code=503, detail="Runner not initialized")
    
    result = await runner.cleanup(namespace=namespace)
    return result


# =============================================================================
# Categories and Severities Reference
# =============================================================================

@app.get("/categories")
async def list_categories():
    """List all exploit categories."""
    return {
        "categories": [
            {"value": c.value, "name": c.name}
            for c in ExploitCategory
        ]
    }


@app.get("/severities")
async def list_severities():
    """List all severity levels."""
    return {
        "severities": [
            {"value": s.value, "name": s.name}
            for s in ExploitSeverity
        ]
    }


# =============================================================================
# CloudEvents Handler Endpoint
# =============================================================================

@app.post("/")
async def handle_cloudevent(request: Request):
    """
    Handle incoming CloudEvents from Knative Triggers.
    
    This endpoint receives CloudEvents and triggers exploit execution or k6 tests based on event type.
    Uses the same shared business logic as API endpoints for consistency and tracing.
    
    Exploit Events:
    - io.homelab.redteam.run: Run specific exploit (payload: {exploit_id, namespace})
    - io.homelab.redteam.run.random: Run RANDOM exploit (payload: {namespace, severity?})
    - io.homelab.redteam.cleanup: Cleanup exploit resources
    - io.homelab.vuln.found: Trigger exploit validation - runs RANDOM exploit if no match!
    
    K6 Test Events (from knative-lambda-operator):
    - io.knative.lambda.lifecycle.function.created: Trigger sequential attack test
    - io.knative.lambda.lifecycle.function.ready: Trigger parallel attack test
    - io.knative.lambda.lifecycle.build.failed: Trigger vulnerability assessment
    - io.knative.lambda.notification.alert.critical: Trigger random chaos test
    - io.knative.lambda.lifecycle.service.scaled: Trigger load test
    - io.homelab.redteam.test.k6: Explicit k6 test trigger (payload: {test_type, namespace?})
    """
    import time
    from opentelemetry import trace
    
    tracer = trace.get_tracer(__name__)
    start_time = time.monotonic()
    
    if runner is None:
        raise HTTPException(status_code=503, detail="Runner not initialized")
    
    # Parse CloudEvent headers
    ce_type = request.headers.get("ce-type", "")
    ce_source = request.headers.get("ce-source", "")
    ce_id = request.headers.get("ce-id", "")
    
    # Track CloudEvent received
    if ce_type:
        record_cloudevent(event_type=ce_type, source=ce_source or "unknown", status="received")
    
    # Create tracing span for CloudEvent processing
    with tracer.start_as_current_span(
        f"cloudevent.{ce_type.replace('.', '_').replace(':', '_')}",
        attributes={
            "cloudevent.type": ce_type,
            "cloudevent.source": ce_source,
            "cloudevent.id": ce_id,
        }
    ) as span:
        logger.info(
            "cloudevent_received",
            ce_type=ce_type,
            ce_source=ce_source,
            ce_id=ce_id,
            source_type="cloudevent",
        )
        
        # Parse event data
        try:
            body = await request.json()
        except Exception as e:
            logger.warning("cloudevent_invalid_json", error=str(e))
            body = {}
            # Track error
            if ce_type:
                duration = time.monotonic() - start_time
                record_cloudevent(event_type=ce_type, source=ce_source or "unknown", status="error", duration=duration)
            raise HTTPException(status_code=400, detail="Invalid JSON in CloudEvent body")
        
        # Route based on event type - use shared business logic
        if ce_type == "io.homelab.redteam.run":
            # Run exploit from event - use same function as API endpoint
            exploit_id = body.get("exploit_id")
            namespace = body.get("namespace")
            
            if not exploit_id:
                logger.error("cloudevent_missing_exploit_id")
                span.set_attribute("cloudevent.error", "missing_exploit_id")
                return JSONResponse(
                    status_code=400,
                    content={"error": "exploit_id required in event data"}
                )
            
            logger.info(
                "cloudevent_triggering_exploit",
                exploit_id=exploit_id,
                namespace=namespace,
            )
            
            # Use shared business logic (same as API endpoint)
            result = await runner.run_exploit(
                exploit_id=exploit_id,
                namespace=namespace,
            )
            
            span.set_attribute("exploit.id", exploit_id)
            span.set_attribute("exploit.status", result.status.value)
            span.set_attribute("cloudevent.duration_ms", (time.monotonic() - start_time) * 1000)
            
            # Emit result event
            await _emit_exploit_result(result, ce_id)
            
            # Track successful processing
            duration = time.monotonic() - start_time
            record_cloudevent(event_type=ce_type, source=ce_source or "unknown", status="success", duration=duration)
            
            return {
                "status": "accepted",
                "exploit_id": exploit_id,
                "result": {
                    "status": result.status.value,
                    "exploit_id": result.exploit_id,
                },
                "event_id": ce_id,
                "duration_ms": duration * 1000,
            }
    
        elif ce_type == "io.homelab.redteam.cleanup":
            # Cleanup exploit resources - use shared business logic
            namespace = body.get("namespace")
            
            logger.info("cloudevent_triggering_cleanup", namespace=namespace)
            
            # Use shared business logic (same as API endpoint)
            cleanup_result = await runner.cleanup(namespace=namespace)
            
            span.set_attribute("cleanup.namespace", namespace or "default")
            duration = time.monotonic() - start_time
            span.set_attribute("cloudevent.duration_ms", duration * 1000)
            
            # Track successful processing
            record_cloudevent(event_type=ce_type, source=ce_source or "unknown", status="success", duration=duration)
            
            return {
                "status": "accepted",
                "cleanup": cleanup_result,
                "event_id": ce_id,
                "duration_ms": duration * 1000,
            }
    
        elif ce_type == "io.homelab.vuln.found":
            # Vulnerability found - trigger exploit validation
            # If no matching exploit found, run a RANDOM exploit!
            vuln_id = body.get("vulnerability_id")
            vuln_type = body.get("vulnerability_type")
            namespace = body.get("namespace")
            
            span.set_attribute("vulnerability.id", vuln_id or "unknown")
            span.set_attribute("vulnerability.type", vuln_type or "unknown")
            
            logger.info(
                "cloudevent_vuln_found",
                vuln_id=vuln_id,
                vuln_type=vuln_type,
            )
            
            # Find matching exploit for this vulnerability
            matching_exploits = [
                e for e in runner.catalog.exploits
                if vuln_type and vuln_type.lower() in e.category.value.lower()
            ]
            
            if matching_exploits:
                # Run first matching exploit
                exploit = matching_exploits[0]
                logger.info("cloudevent_running_matching_exploit", exploit_id=exploit.id)
                span.set_attribute("exploit.matched", True)
            else:
                # NO MATCHING EXPLOIT - RUN RANDOM EXPLOIT! ðŸŽ²
                exploit = random.choice(runner.catalog.exploits)
                logger.warning(
                    "cloudevent_no_matching_exploit_running_random",
                    vuln_type=vuln_type,
                    random_exploit_id=exploit.id,
                )
                span.set_attribute("exploit.matched", False)
                span.set_attribute("exploit.random", True)
            
            # Use shared business logic (same as API endpoint)
            result = await runner.run_exploit(exploit.id, namespace=namespace)
            
            span.set_attribute("exploit.id", exploit.id)
            span.set_attribute("exploit.status", result.status.value)
            span.set_attribute("cloudevent.duration_ms", (time.monotonic() - start_time) * 1000)
            
            await _emit_exploit_result(result, ce_id)
            
            # Track successful processing
            duration = time.monotonic() - start_time
            record_cloudevent(event_type=ce_type, source=ce_source or "unknown", status="success", duration=duration)
            
            return {
                "status": "accepted",
                "vulnerability_id": vuln_id,
                "exploit_run": {
                    "exploit_id": exploit.id,
                    "status": result.status.value,
                    "was_random": len(matching_exploits) == 0,
                },
                "event_id": ce_id,
                "duration_ms": duration * 1000,
            }
    
        elif ce_type == "io.homelab.redteam.run.random":
            # Explicit request to run a RANDOM exploit - use shared business logic
            namespace = body.get("namespace")
            severity_filter = body.get("severity")  # Optional: filter by severity
            
            span.set_attribute("exploit.random", True)
            if severity_filter:
                span.set_attribute("exploit.severity_filter", severity_filter)
            
            logger.info(
                "cloudevent_run_random_exploit",
                namespace=namespace,
                severity_filter=severity_filter,
            )
            
            # Filter exploits by severity if specified
            available_exploits = runner.catalog.exploits
            if severity_filter:
                from shared.types import ExploitSeverity
                try:
                    sev = ExploitSeverity(severity_filter.lower())
                    available_exploits = [e for e in available_exploits if e.severity == sev]
                except ValueError:
                    logger.warning("cloudevent_invalid_severity", severity=severity_filter)
            
            if not available_exploits:
                span.set_attribute("cloudevent.error", "no_exploits_available")
                return JSONResponse(
                    status_code=400,
                    content={"error": f"No exploits available for severity: {severity_filter}"}
                )
            
            # Pick random exploit
            exploit = random.choice(available_exploits)
            logger.info("cloudevent_running_random_exploit", exploit_id=exploit.id, severity=exploit.severity.value)
            
            # Use shared business logic (same as API endpoint)
            result = await runner.run_exploit(exploit.id, namespace=namespace)
            
            span.set_attribute("exploit.id", exploit.id)
            span.set_attribute("exploit.status", result.status.value)
            span.set_attribute("cloudevent.duration_ms", (time.monotonic() - start_time) * 1000)
            
            await _emit_exploit_result(result, ce_id)
            
            return {
                "status": "accepted",
                "exploit_run": {
                    "exploit_id": exploit.id,
                    "exploit_name": exploit.name,
                    "severity": exploit.severity.value,
                    "status": result.status.value,
                },
                "event_id": ce_id,
                "duration_ms": (time.monotonic() - start_time) * 1000,
            }
        
        elif ce_type.startswith("io.knative.lambda."):
            # Events from knative-lambda-operator - trigger k6 tests
            span.set_attribute("cloudevent.operator_event", True)
            result = await _handle_operator_event(ce_type, body, ce_id, span)
            span.set_attribute("cloudevent.duration_ms", (time.monotonic() - start_time) * 1000)
            return result
        
        elif ce_type == "io.homelab.redteam.test.k6":
            # Explicit k6 test trigger - use shared business logic
            test_type = body.get("test_type")
            namespace = body.get("namespace", "agent-redteam")
            
            span.set_attribute("k6.test_type", test_type or "unknown")
            
            if not test_type:
                span.set_attribute("cloudevent.error", "missing_test_type")
                return JSONResponse(
                    status_code=400,
                    content={"error": "test_type required (smoke, attack-sequential, attack-parallel, vulnerability-assessment, random-chaos)"}
                )
            
            logger.info("cloudevent_triggering_k6_test", test_type=test_type, namespace=namespace)
            
            # Use shared business logic (same as API endpoint)
            success, message = await runner.create_k6_testrun(
                test_type=test_type,
                namespace=namespace,
                env_vars=body.get("env_vars"),
            )
            
            span.set_attribute("k6.success", success)
            span.set_attribute("cloudevent.duration_ms", (time.monotonic() - start_time) * 1000)
            
            if success:
                return {
                    "status": "accepted",
                    "test_type": test_type,
                    "message": message,
                    "event_id": ce_id,
                    "duration_ms": (time.monotonic() - start_time) * 1000,
                }
            else:
                return JSONResponse(
                    status_code=500,
                    content={"error": message}
                )
        
        else:
            logger.warning("cloudevent_unknown_type", ce_type=ce_type)
            span.set_attribute("cloudevent.error", "unknown_type")
            duration = time.monotonic() - start_time
            span.set_attribute("cloudevent.duration_ms", duration * 1000)
            
            # Track unknown event type
            record_cloudevent(event_type=ce_type or "unknown", source=ce_source or "unknown", status="unknown_type", duration=duration)
            
            return JSONResponse(
                status_code=400,
                content={"error": f"Unknown event type: {ce_type}"}
            )


async def _handle_operator_event(ce_type: str, body: dict, ce_id: str, span=None) -> dict:
    """
    Handle events from knative-lambda-operator and trigger appropriate k6 tests.
    
    Event mappings:
    - function.created -> sequential attack
    - function.ready -> parallel attack
    - build.failed -> vulnerability assessment
    - alert.critical -> random chaos
    - service.scaled -> load test (if implemented)
    """
    if runner is None:
        raise HTTPException(status_code=503, detail="Runner not initialized")
    
    # Map operator events to k6 test types
    event_to_test = {
        "io.knative.lambda.lifecycle.function.created": "attack-sequential",
        "io.knative.lambda.lifecycle.function.ready": "attack-parallel",
        "io.knative.lambda.lifecycle.build.failed": "vulnerability-assessment",
        "io.knative.lambda.notification.alert.critical": "random-chaos",
        "io.knative.lambda.lifecycle.service.scaled": "attack-parallel",  # Use parallel for load
    }
    
    test_type = event_to_test.get(ce_type)
    
    if not test_type:
        logger.info("cloudevent_operator_event_no_mapping", ce_type=ce_type)
        return {
            "status": "accepted",
            "message": f"Event {ce_type} received but no k6 test mapping configured",
        }
    
    namespace = body.get("namespace", "agent-redteam")
    function_name = body.get("function_name") or body.get("name", "unknown")
    
    logger.info(
        "cloudevent_operator_triggering_k6",
        ce_type=ce_type,
        test_type=test_type,
        function_name=function_name,
        namespace=namespace,
    )
    
    # Create k6 test run
    success, message = await runner.create_k6_testrun(
        test_type=test_type,
        namespace=namespace,
        env_vars={
            "TRIGGER_EVENT": ce_type,
            "TRIGGER_FUNCTION": function_name,
            "TRIGGER_EVENT_ID": ce_id,
        },
    )
    
    if success:
        return {
            "status": "accepted",
            "event_type": ce_type,
            "test_type": test_type,
            "function_name": function_name,
            "message": message,
        }
    else:
        return JSONResponse(
            status_code=500,
            content={"error": f"Failed to create k6 test: {message}"}
        )


async def _emit_exploit_result(result: Any, correlation_id: Optional[str] = None) -> None:
    """
    Emit CloudEvent for exploit result.
    
    Note: In a real implementation, this would send to the broker.
    For now, we just log it. The operator's eventing manager handles
    actual event emission via the intents configuration.
    """
    event_type = {
        "success": "io.homelab.exploit.success",
        "blocked": "io.homelab.exploit.blocked",
        "failed": "io.homelab.exploit.failed",
        "error": "io.homelab.exploit.error",
    }.get(result.status.value, "io.homelab.exploit.executed")
    
    logger.info(
        "exploit_result_event",
        event_type=event_type,
        exploit_id=result.exploit_id,
        status=result.status.value,
        correlation_id=correlation_id,
    )
    
    # TODO: Send to broker if EMIT_EVENTS is enabled
    # For now, the operator's Trigger will forward based on intents


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
